#!/usr/bin/env python3
"""
Script: robots_generator.py
Purpose: Generate robots.txt for web projects
Usage: python robots_generator.py <project_path> [--base-url https://example.com] [--output robots.txt]
"""

import sys
import argparse
from pathlib import Path
from typing import List, Dict
import json

# Fix Windows console encoding
try:
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
except AttributeError:
    pass


# ============================================================================
#  CONFIGURATION
# ============================================================================

# Default disallow patterns by project type
DEFAULT_DISALLOW = {
    "nextjs": ["/api/", "/_next/", "/admin/", "/private/"],
    "astro": ["/api/", "/admin/", "/private/"],
    "general": ["/admin/", "/private/", "/api/", "/cgi-bin/"]
}

# Patterns that should always be disallowed
ALWAYS_DISALLOW = [
    "/wp-admin/",
    "/wp-includes/",
    "/*.json$",
    "/*?*",
]

# Patterns that should never be disallowed
NEVER_DISALLOW = [
    "/",
    "/sitemap.xml",
    "/robots.txt"
]


# ============================================================================
#  DETECTION
# ============================================================================

def detect_project_type(project_path: Path) -> str:
    """Detect project type."""
    if (project_path / "next.config.js").exists() or (project_path / "next.config.mjs").exists():
        return "nextjs"
    if (project_path / "astro.config.mjs").exists():
        return "astro"
    return "general"


def detect_disallow_paths(project_path: Path, project_type: str) -> List[str]:
    """Detect paths that should be disallowed."""
    disallow = set(DEFAULT_DISALLOW.get(project_type, DEFAULT_DISALLOW["general"]))

    # Check for common admin/private directories
    for pattern in ["admin", "private", "internal", "dashboard", "backend"]:
        if (project_path / pattern).exists():
            disallow.add(f"/{pattern}/")

        # Check in app directory for Next.js
        if (project_path / "app" / pattern).exists():
            disallow.add(f"/{pattern}/")

    # Check for API routes
    if (project_path / "api").exists() or (project_path / "app" / "api").exists():
        disallow.add("/api/")

    # Check for pages directory API routes
    if (project_path / "pages" / "api").exists():
        disallow.add("/api/")

    return sorted(disallow)


def detect_allow_paths(project_path: Path) -> List[str]:
    """Detect paths that should be explicitly allowed."""
    allow = []

    # Allow static assets that might be in disallowed directories
    if (project_path / "public").exists():
        for item in (project_path / "public").iterdir():
            if item.is_file() and item.suffix in [".ico", ".png", ".svg", ".jpg"]:
                allow.append(f"/{item.name}")

    return allow


# ============================================================================
#  GENERATION
# ============================================================================

def generate_robots_txt(
    base_url: str,
    disallow: List[str],
    allow: List[str],
    sitemap: bool = True,
    crawl_delay: int = None
) -> str:
    """Generate robots.txt content."""
    lines = [
        "# robots.txt",
        "# Generated by Maestro SEO Tools",
        "",
        "User-agent: *"
    ]

    # Add allow rules first (more specific)
    for path in allow:
        lines.append(f"Allow: {path}")

    # Add disallow rules
    for path in disallow:
        lines.append(f"Disallow: {path}")

    # Add crawl delay if specified
    if crawl_delay:
        lines.append(f"Crawl-delay: {crawl_delay}")

    lines.append("")

    # Add sitemap reference
    if sitemap:
        sitemap_url = base_url.rstrip("/") + "/sitemap.xml"
        lines.append(f"Sitemap: {sitemap_url}")

    # Add specific bot rules (optional, commonly used)
    lines.extend([
        "",
        "# Block AI training crawlers (optional)",
        "User-agent: GPTBot",
        "Disallow: /",
        "",
        "User-agent: ChatGPT-User",
        "Disallow: /",
        "",
        "User-agent: CCBot",
        "Disallow: /",
        "",
        "User-agent: anthropic-ai",
        "Disallow: /",
        "",
        "# Allow good bots full access",
        "User-agent: Googlebot",
        "Allow: /",
        "",
        "User-agent: Bingbot",
        "Allow: /",
    ])

    return "\n".join(lines)


def generate_minimal_robots_txt(base_url: str) -> str:
    """Generate minimal robots.txt."""
    return f"""# robots.txt
User-agent: *
Allow: /

Sitemap: {base_url.rstrip('/')}/sitemap.xml
"""


# ============================================================================
#  MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Generate robots.txt for web projects"
    )
    parser.add_argument("project_path", nargs="?", default=".", help="Project directory")
    parser.add_argument("--base-url", "-b", required=True, help="Base URL")
    parser.add_argument("--output", "-o", help="Output file path")
    parser.add_argument("--minimal", action="store_true", help="Generate minimal robots.txt")
    parser.add_argument("--no-sitemap", action="store_true", help="Don't include sitemap reference")
    parser.add_argument("--crawl-delay", type=int, help="Crawl delay in seconds")
    parser.add_argument("--disallow", nargs="*", help="Additional paths to disallow")
    parser.add_argument("--allow", nargs="*", help="Additional paths to allow")

    args = parser.parse_args()

    project_path = Path(args.project_path).resolve()

    if not project_path.is_dir():
        print(f"Error: Not a directory: {project_path}")
        sys.exit(1)

    # Generate content
    if args.minimal:
        content = generate_minimal_robots_txt(args.base_url)
    else:
        # Detect project type
        project_type = detect_project_type(project_path)
        print(f"Detected project type: {project_type}")

        # Detect paths
        disallow = detect_disallow_paths(project_path, project_type)
        allow = detect_allow_paths(project_path)

        # Add custom paths
        if args.disallow:
            disallow.extend(args.disallow)
        if args.allow:
            allow.extend(args.allow)

        content = generate_robots_txt(
            args.base_url,
            disallow,
            allow,
            sitemap=not args.no_sitemap,
            crawl_delay=args.crawl_delay
        )

    # Determine output path
    if args.output:
        output_path = Path(args.output)
    else:
        # Try to find public directory
        public_dir = project_path / "public"
        if public_dir.exists():
            output_path = public_dir / "robots.txt"
        else:
            output_path = project_path / "robots.txt"

    # Write output
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)

    print(f"robots.txt saved to: {output_path}")
    print("\nContent:")
    print("-" * 40)
    print(content)


if __name__ == "__main__":
    main()
